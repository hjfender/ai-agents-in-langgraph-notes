{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from tavily import TavilyClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Essay Writer Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<img src=\"Essay Writer Design.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay. \\\n",
    "Write such an outline for the user provided topic. Give an outline of the essay along with any relevant notes \\\n",
    "or instructions for the sections.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays.\\\n",
    "Generate the best essay possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
    "Utilize all the information below as needed: \n",
    "\n",
    "------\n",
    "\n",
    "{content}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following essay. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as outlined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries(BaseModel):\n",
    "    queries: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT), \n",
    "        HumanMessage(content=state['task'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"plan\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_plan_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join(state['content'] or [])\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=WRITER_PROMPT.format(content=content)\n",
    "        ),\n",
    "        user_message\n",
    "        ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content, \n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT), \n",
    "        HumanMessage(content=state['draft'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"critique\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_critique_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "        HumanMessage(content=state['critique'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.set_entry_point(\"planner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    should_continue, \n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "\n",
    "# Image(graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planner': {'plan': 'I. Introduction\\n    A. Brief overview of Langchain and Langsmith\\n    B. Thesis statement: Exploring the differences between Langchain and Langsmith\\n\\nII. Langchain\\n    A. Definition and explanation\\n    B. Key features and characteristics\\n    C. Use cases and applications\\n    D. Advantages and disadvantages\\n\\nIII. Langsmith\\n    A. Definition and explanation\\n    B. Key features and characteristics\\n    C. Use cases and applications\\n    D. Advantages and disadvantages\\n\\nIV. Comparison between Langchain and Langsmith\\n    A. Technology stack\\n    B. Scalability\\n    C. Security\\n    D. Performance\\n    E. Adoption and popularity\\n\\nV. Conclusion\\n    A. Recap of main differences between Langchain and Langsmith\\n    B. Implications for the future of blockchain technology\\n    C. Final thoughts and recommendations\\n\\nNotes:\\n- Ensure to provide clear definitions and explanations of both Langchain and Langsmith.\\n- Include specific examples of real-world applications for each technology.\\n- Use data and statistics to support the comparison between the two technologies.\\n- Conclude with insights on the potential impact of Langchain and Langsmith on the blockchain industry.'}}\n",
      "{'research_plan': {'content': ['If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while Langchain excels at managing and scaling model workflows, Langsmith is designed for those times when you need deep visibility and control over large, complex AI systems in production. But if you’re managing a complex AI pipeline with multiple models that need debugging and orchestrating, Langsmith’s capabilities become essential. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, Langsmith’s advanced debugging and orchestration features will be indispensable. Additionally, if you’re working on cross-platform model deployments — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'LangChain vs LangSmith: Understanding the Differences, Pros, and Cons | by Ajay Verma | GoPenAI LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. In this blog, we’ll delve into the differences between LangChain and LangSmith, their pros and cons, and when to use each one. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities.', 'If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while Langchain excels at managing and scaling model workflows, Langsmith is designed for those times when you need deep visibility and control over large, complex AI systems in production. But if you’re managing a complex AI pipeline with multiple models that need debugging and orchestrating, Langsmith’s capabilities become essential. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, Langsmith’s advanced debugging and orchestration features will be indispensable. Additionally, if you’re working on cross-platform model deployments — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'LangChain vs LangSmith: Understanding the Differences, Pros, and Cons | by Ajay Verma | GoPenAI LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. In this blog, we’ll delve into the differences between LangChain and LangSmith, their pros and cons, and when to use each one. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities.', 'Here’s the deal: LangChain is like building the entire car, while LangSmith is your diagnostic tool to ensure that car runs smoothly. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', \"LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. You might think of LangSmith as LangChain's counterpart, but it takes things further by focusing on managing, debugging, and orchestrating AI and ML models. LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. In short, while LangChain excels at managing and scaling model workflows, LangSmith is designed for when you need deep visibility and control over large, complex AI systems in production. If you're debugging complex AI models or managing large-scale workflows with multiple moving parts, LangSmith's advanced debugging and orchestration features will be indispensable.\"]}}\n",
      "{'generate': {'draft': \"**Title: Exploring the Contrasts Between LangChain and LangSmith**\\n\\nI. Introduction\\nLangChain and LangSmith are two essential tools in the realm of AI model development and deployment. While both serve distinct purposes, understanding their unique features is crucial for maximizing their potential in various applications.\\n\\nII. LangChain\\nLangChain serves as a foundational tool for building and scaling AI model workflows. It is ideal for early-stage prototyping and smaller applications, offering a comprehensive platform for initial development. However, its capabilities may be limited when it comes to managing large-scale, production-ready applications that require advanced debugging and monitoring.\\n\\nIII. LangSmith\\nOn the other hand, LangSmith is designed to provide deep visibility and control over complex AI systems in production. It excels in debugging and orchestrating AI models at scale, making it indispensable for managing intricate workflows with multiple moving parts. Additionally, LangSmith offers better orchestration and monitoring tools for cross-platform model deployments, ensuring smooth operation across different environments.\\n\\nIV. Comparison between LangChain and LangSmith\\nA. Technology Stack: LangChain focuses on managing and scaling model workflows, while LangSmith emphasizes debugging and orchestrating AI models.\\nB. Scalability: LangChain is suitable for early-stage prototyping and small-scale applications, whereas LangSmith is better equipped for large-scale, production-ready systems.\\nC. Security: Both tools prioritize security, but LangSmith's advanced debugging features enhance system integrity.\\nD. Performance: LangChain streamlines workflow management, while LangSmith optimizes model performance through monitoring and debugging.\\nE. Adoption and Popularity: LangChain may be more popular for initial development stages, while LangSmith gains traction for managing complex AI systems in production.\\n\\nV. Conclusion\\nIn conclusion, LangChain and LangSmith play vital roles in AI model development, each catering to specific needs and requirements. Understanding the differences between these tools is essential for leveraging their capabilities effectively. As the demand for advanced AI systems grows, the combined use of LangChain and LangSmith can lead to more efficient and reliable AI model deployments, shaping the future of AI technology.\", 'revision_number': 2}}\n",
      "{'reflect': {'critique': \"**Critique:**\\n\\n1. **Introduction:**\\n   - The introduction provides a clear overview of the topic but could benefit from a more engaging hook to capture the reader's interest.\\n   - Consider adding a brief explanation of why understanding LangChain and LangSmith is important in the context of AI model development.\\n\\n2. **Content:**\\n   - The essay effectively outlines the key features and purposes of LangChain and LangSmith.\\n   - The comparison between the two tools is well-structured and provides a comprehensive analysis of their differences in technology stack, scalability, security, performance, and adoption.\\n   - It would be beneficial to include specific examples or case studies to illustrate how each tool is used in real-world scenarios.\\n\\n3. **Depth and Detail:**\\n   - While the essay covers the basics of LangChain and LangSmith, it could be enhanced by delving deeper into the technical aspects of each tool.\\n   - Consider providing more in-depth explanations of how LangChain and LangSmith function, including specific features, algorithms, or methodologies they employ.\\n\\n4. **Recommendations:**\\n   - Expand on the discussion of security features in LangChain and LangSmith. Provide examples of how each tool ensures data privacy and system integrity.\\n   - Include a section on the limitations of LangChain and LangSmith to provide a more balanced perspective on their capabilities.\\n   - Add a section on future developments or trends in AI model development that may impact the use of LangChain and LangSmith.\\n\\n5. **Style and Clarity:**\\n   - The essay is well-organized and easy to follow, with clear headings for each section.\\n   - Consider incorporating more transitions between paragraphs to improve the flow of the essay.\\n   - Use more descriptive language and examples to make the content more engaging and informative for the reader.\\n\\n**Overall, this essay provides a solid overview of LangChain and LangSmith, but it would benefit from additional depth, examples, and analysis to enhance the reader's understanding of these tools and their implications in AI model development.**\"}}\n",
      "{'research_critique': {'content': ['If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while Langchain excels at managing and scaling model workflows, Langsmith is designed for those times when you need deep visibility and control over large, complex AI systems in production. But if you’re managing a complex AI pipeline with multiple models that need debugging and orchestrating, Langsmith’s capabilities become essential. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, Langsmith’s advanced debugging and orchestration features will be indispensable. Additionally, if you’re working on cross-platform model deployments — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'LangChain vs LangSmith: Understanding the Differences, Pros, and Cons | by Ajay Verma | GoPenAI LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. In this blog, we’ll delve into the differences between LangChain and LangSmith, their pros and cons, and when to use each one. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities.', 'If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while Langchain excels at managing and scaling model workflows, Langsmith is designed for those times when you need deep visibility and control over large, complex AI systems in production. But if you’re managing a complex AI pipeline with multiple models that need debugging and orchestrating, Langsmith’s capabilities become essential. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, Langsmith’s advanced debugging and orchestration features will be indispensable. Additionally, if you’re working on cross-platform model deployments — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'LangChain vs LangSmith: Understanding the Differences, Pros, and Cons | by Ajay Verma | GoPenAI LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. In this blog, we’ll delve into the differences between LangChain and LangSmith, their pros and cons, and when to use each one. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities.', 'Here’s the deal: LangChain is like building the entire car, while LangSmith is your diagnostic tool to ensure that car runs smoothly. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', \"LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. You might think of LangSmith as LangChain's counterpart, but it takes things further by focusing on managing, debugging, and orchestrating AI and ML models. LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. In short, while LangChain excels at managing and scaling model workflows, LangSmith is designed for when you need deep visibility and control over large, complex AI systems in production. If you're debugging complex AI models or managing large-scale workflows with multiple moving parts, LangSmith's advanced debugging and orchestration features will be indispensable.\", 'LangChain is an open source framework that enables software developers working with artificial intelligence (AI) and its machine learning subset to combine large language models with other external components to develop LLM-powered applications. LangChain is a framework that simplifies the process of creating generative AI application interfaces. LangChain includes prompt template modules that enable developers to create structured prompts for LLMs. These templates can incorporate examples and specify output formats, facilitating smoother interactions and more accurate responses from the models. The LangChain framework helps developers create LLM-powered applications by offering tools that build intricate workflows and integrate different components. Use LangChain and vector search on Amazon DocumentDB to build a generative AI ...', \"So, even if you’re developing AI-driven chatbots, automating workflows, or analyzing large volumes of text, LangChain provides the tools to seamlessly integrate LLMs into real-world applications. LangChain offers a structured approach to building AI applications by enhancing context management, streamlining multi-step processes, and allowing easy integration with external resources. With solutions like LangChain, AI models can store past interactions and recall previous solutions, allowing for more consistent and intelligent responses, helping businesses provide faster and more efficient customer support while escalating complex cases to human agents only when necessary. Through combining memory management, real-time data access, and structured workflows, LangChain makes AI applications smarter, more efficient, and more intuitive. Our team of AI specialists and LangChain developer can help you integrate seamlessly into your projects, whether you're looking to build intelligent chatbots, automate workflows, or enhance data-driven applications.\", 'This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications. From its seamless integration with LangChain to its robust Cookbook filled with real-world examples, LangSmith is a game-changer. Logging Assertions as Feedback (opens in a new tab): Convert CI test assertions into LangSmith feedback. Exporting LLM Runs and Feedback (opens in a new tab): Extract and interpret LangSmith LLM run data for various analytical platforms. While LangSmith is focused on building and managing LLM applications, LangChain serves as a framework for developing language models. LangSmith integrates seamlessly with LangChain, offering a unified platform for all your LLM needs.', \"Build production-grade LLM applications using LangSmith | by Rahul Pandey | DSciEr | Medium LangSmith 101 Build production-grade LLM applications using LangSmith Published in LangSmith by LangChain is a platform that streamlines the development of LLM applications with debugging, testing, evaluating, and monitoring. Building an application powered by a large language model (LLM) differs slightly from building a regular advanced analytics application. Typically, the first preference in LLM applications is to use pre-built models, as training LLMs can be expensive. So, you need a workflow that efficiently creates and tests these prompts to see how well your LLM application is doing without drowning in manual checks. This is where LangSmith comes into the picture. Visualize Components: See how your app's chains, LLMs, and retrievers work. Published in DSciEr -------------------\", \"Build a simple LLM application with chat models and prompt templates How to use tools in a chain How to use few shot examples in chat models How to add ad-hoc tool calling capability to LLMs and Chat Models How to use prompting alone (no tool calling) to do extraction How to migrate from legacy LangChain agents to LangGraph How to use LangChain with different Pydantic versions How to use chat models to call tools How to pass tool outputs to chat models How to use few-shot prompting with tool calling If you're building applications that access external resources like file systems, APIs or databases, consider speaking with your company's security team to determine how to best design and secure your applications. Please report security vulnerabilities associated with LangSmith by email to security@langchain.dev.\", \"LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. You might think of LangSmith as LangChain's counterpart, but it takes things further by focusing on managing, debugging, and orchestrating AI and ML models. LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. In short, while LangChain excels at managing and scaling model workflows, LangSmith is designed for when you need deep visibility and control over large, complex AI systems in production. If you're debugging complex AI models or managing large-scale workflows with multiple moving parts, LangSmith's advanced debugging and orchestration features will be indispensable.\"]}}\n",
      "{'generate': {'draft': \"**Title: Exploring the Contrasts Between LangChain and LangSmith**\\n\\nI. Introduction\\nLangChain and LangSmith are two pivotal tools developed by LangChain, each serving distinct purposes in the realm of Large Language Model (LLM) applications. This essay aims to delve into the disparities between LangChain and LangSmith to provide a comprehensive understanding of their functionalities.\\n\\nII. LangChain\\nLangChain is a framework designed for early-stage prototyping and small-scale applications in the LLM domain. It simplifies the integration of large language models with external components, enabling the development of LLM-powered applications. With prompt template modules and structured workflows, LangChain facilitates the creation of generative AI interfaces. While advantageous for context management and multi-step processes, LangChain may lack the advanced debugging and monitoring capabilities required for large-scale production-ready applications.\\n\\nIII. LangSmith\\nIn contrast, LangSmith offers a unified platform tailored for managing, debugging, testing, and monitoring LLM applications at scale. It provides advanced features for orchestrating AI and ML models, ensuring smooth operations in complex AI systems. LangSmith's focus on deep visibility and control over intricate AI systems makes it indispensable for debugging complex models and managing large-scale workflows with multiple moving parts.\\n\\nIV. Comparison between LangChain and LangSmith\\nA. Technology Stack: LangChain focuses on integrating LLMs with external components, while LangSmith emphasizes debugging and monitoring capabilities.\\nB. Scalability: LangChain is ideal for small-scale applications, whereas LangSmith excels in managing large-scale, production-ready workflows.\\nC. Security: Both tools prioritize security, but LangSmith's advanced monitoring tools enhance system security.\\nD. Performance: LangChain streamlines AI application development, while LangSmith ensures optimal performance through advanced debugging features.\\nE. Adoption and Popularity: LangSmith's advanced debugging and orchestration features have garnered popularity for managing complex AI systems in production.\\n\\nV. Conclusion\\nIn conclusion, LangChain and LangSmith cater to different stages and requirements of LLM development. While LangChain is suitable for early-stage prototyping, LangSmith shines in managing large-scale, production-ready applications. Understanding the distinctions between these tools is crucial for leveraging their capabilities effectively in the evolving landscape of AI applications.\", 'revision_number': 3}}\n"
     ]
    }
   ],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for s in graph.stream({\n",
    "    'task': \"what is the difference between langchain and langsmith\",\n",
    "    \"max_revisions\": 2,\n",
    "    \"revision_number\": 1,\n",
    "}, thread):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from helper import ewriter, writer_gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiAgent = ewriter()\n",
    "# app = writer_gui(MultiAgent.graph)\n",
    "# app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agents-in-langgraph-notes-ufgr41S4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
